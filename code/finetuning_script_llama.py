import torch
import transformers
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    DataCollatorForLanguageModeling,
    TrainingArguments,
    Trainer,
    pipeline,
    logging,
)

from accelerate import Accelerator
from datasets import load_dataset
import pandas as pd
import numpy as np
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
import re
from sklearn import metrics
from trl import SFTTrainer, DataCollatorForCompletionOnlyLM
import time

d = {0:"Satisfaction", 1:"Feasibility", 2:"Consistency", 3:"Completeness", 4:"Trust", 5:"Understandability", 6:"Fairness", 7:"Complexity"}
sys = """You are evaluating counterfactual explanations generated by AI.  Counterfactual explanations explain what parameters of a situation should have been different for the outcome to have been different. You are not expected to provide reasoning or explanation and should answer with the appropriate value from the set ["low", "medium", "high"]."""
sysend =  "The following is the counterfactual explanation. "

def match1(i):
    if i==0: 
        return " Please rate as 'low' (very unsatisfied), 'medium' or 'high' (completely satisfied), how satisfied would you be with such an explanation: "
    if i==1: 
        return " Please rate as 'low' (very unfeasible), 'medium' or 'high' (completely feasible), how feasible is this explanation: "
    if i==2: 
        return " Please rate as 'low' (very unconsistent), 'medium' or 'high' (completely consistent), how consistent is this explanation: "
    if i==3:
        return " Please rate as 'low' (very incomplete), 'medium' or 'high' (fully complete), how complete is this explanation: "
    if i==4:
        return " Please rate as 'low' (not at all), 'medium' or 'high' (completely), how much would you trust this explanation: "
    if i==5:
        return " Please rate as 'low' (incomprehensible), 'medium' or 'high' (very understandable), how understandable is this explanation: "
    if i==6:
        return " Please rate as 'low' (completely biased), 'medium' or 'high' (completely fair), how fair is this explanation: "
    if i==7:
        return " Please rate as 'low' (too easy), 'medium' (ideally complex) or 'high' (too complex), how complex is this explanation: "
    return " Please rate as 'low' (very unsatisfied), 'medium' or 'high' (completely satisfied), how satisfied would you be with such an explanation: "

def match2(i):
    if i==0:
        return " The definition of satisfaction: this scenario effectively explains how to reach a different outcome. "
    if i==1:
        return " The definition of feasibility: the actions suggested by the explanation are practical, realistic to implement and actionable. "
    if i==2:
        return " The definition of consistency: the parts of the explanation do not contradict each other. "
    if i==3:
        return " The definition of completeness: the explanation is sufficient in explaining how to achieve the desired outcome. "
    if i==4:
        return " The definition of trust: I believe that the suggested changes would bring about the desired outcome. "
    if i==5:
        return " The definition of understandability:  I feel like I understood the phrasing of the explanation well. "
    if i==6:
        return " The definition of fairness: the explanation is unbiased towards different user groups and does not operate on sensitive features. "
    if i==7:
        return " The definition of complexity: the explanation has an appropriate level of detail and complexity - not too simple, yet not overly complex."
    return " The definition of satisfaction: this scenario effectively explains how to reach a different outcome. "
        
        
def prompt(s,i,answer):
    global sys
    global sysend
    prompt = s + match1(i)
    if(answer!=None):
        final = [
                    {"role":"system", "content":sys + match2(i) + sysend},
                    {"role":"user", "content":prompt},
                    {"role":"assistant", "content": answer}
                ]
    else:
        final = [
                    {"role":"system", "content":sys + match2(i) + sysend},
                    {"role":"user", "content":prompt}
                ]
    return final

access_token = "personal_huggingface_token"

base_model = "meta-llama/Meta-Llama-3-70B-Instruct"
base_model = "meta-llama/Meta-Llama-3-8B-Instruct"
base_model = "meta-llama/Meta-Llama-3.1-8B-Instruct"


pad_length = 600

tokenizer = AutoTokenizer.from_pretrained(base_model, token=access_token, padding_side="right", add_eos_token=False, add_bos_token=True, max_length=pad_length, use_fast=False)
tokenizer.pad_token_id = 128001

#For averaged scores
person = "answer"

#For specific participant modelling
#person = "answer_100"
#person = "answer_310"
#person = "answer_356"
#person = "answer_364"

def preprocess_function(example, train):
    if(train):
        p = prompt(example["prompt"], example["type"], example[person])
        pre_token = tokenizer.apply_chat_template(p, tokenize=False, add_generation_prompt=False)
        example = tokenizer(pre_token, padding="max_length", max_length=pad_length, return_attention_mask=True)
    else:
        p = prompt(example["prompt"], example["type"], None)
        pre_token = tokenizer.apply_chat_template(p, tokenize=False, add_generation_prompt=True)
        example = tokenizer(pre_token, padding=False)
    
    example["prompt"] = pre_token
    return example

data = load_dataset("csv", data_files={"train":"trainset.csv", "test":"testset.csv"})
    
data["train"] = data["train"].map(preprocess_function, fn_kwargs={"train":True} , batched=False)
data["test"] = data["test"].map(preprocess_function, fn_kwargs={"train":False} , batched=False)

accelerator = Accelerator()

quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

model = AutoModelForCausalLM.from_pretrained(
    base_model,
    quantization_config=quant_config,
    device_map="auto",
    token=access_token
)

model.config.pad_token_id = 128001
model.resize_token_embeddings(len(tokenizer))

peft_params = LoraConfig(
    lora_alpha=64,
    lora_dropout=0.1,
    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],
    r=32,
    bias="none",
    task_type="CAUSAL_LM",
)



model.gradient_checkpointing_enable()
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, peft_params)

model.config.use_cache = False
model.config.pretraining_tp = 1


#Tested hyperparameter combinations

#epo = 3 #Number of epochs for fine-tuning
#lr = 1e-4 #Learning rate for fine-tuning

#epo = 4
#lr = 2e-4
#lr = 1e-4

epo = 5 
lr = 5e-5 
#lr = 1e-4
#lr = 2e-4
#lr = 8e-5

#epo = 6
#lr = 5e-5


training_args = transformers.TrainingArguments(
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=epo,
    learning_rate=lr,
    logging_steps=15,
    eval_steps=25,
    save_total_limit=4,
    output_dir="./results",
    save_strategy='epoch',
    evaluation_strategy='steps',
)

response_template = "<|start_header_id|>assistant<|end_header_id|>"
data_collator=DataCollatorForCompletionOnlyLM(tokenizer=tokenizer, response_template = response_template)


trainer = SFTTrainer(
    model=model,
    train_dataset=data["train"],
    eval_dataset=data["test"],
    tokenizer=tokenizer,
    args=training_args,
    dataset_text_field="prompt",
    max_seq_length=600,
    data_collator=data_collator
)

trainer.train()

accuracy_scores = []

print("Starting testing")
model.config.use_cache = True
current = time.time()
outps = []
for j in range(4):
    results = []
    for i,pr in enumerate(data["test"]["prompt"]):
        inputs = tokenizer(pr, return_tensors="pt").to("cuda")
        model_output = model.generate(**inputs, temperature=1, do_sample=True, max_new_tokens=60)
        results.append([tokenizer.batch_decode(model_output, skip_special_tokens=True),data["test"][person][i], data["test"]["type"][i]])
        
    for i,(x,y,z) in enumerate(results):
        an = re.search("high|medium|low",x[0].split("assistant")[1].lower())
        result = "Failed"
        if(an!=None):
            result=an.group()
        results[i].append(result)

    answe = np.array(results, dtype=object)
    
    confusion_matrix = metrics.confusion_matrix(answe[:,1], answe[:,3], labels=["low", "medium", "high"])
    print(confusion_matrix)
    
    acc = metrics.accuracy_score(answe[:,1], answe[:,3])
    accuracy_scores.append(acc)
    
    outps = outps + results
    
print("Type: " + person)
print(accuracy_scores)
print("Hyperparameters: " + str(epo) + ", " + str(lr))

print("Average time per answer: " + str((time.time()-current)/(4*len(data["test"]))))

pd.DataFrame(np.array(outps, dtype=object)).to_csv("results_200_llama_" + str(epo) + "_" + str(lr) + ".csv" )
